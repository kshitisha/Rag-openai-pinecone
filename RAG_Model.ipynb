{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai pinecone-client==3.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JVS8zIA5cD3K",
        "outputId": "82097f04-0049-4d18-8f91-d042c5506e67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: pinecone-client==3.0.0 in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (2025.6.15)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (4.14.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai pinecone-client==3.0.0 langchain tiktoken numpy pandas\n",
        "!pip install sentence-transformers faiss-cpu python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XkiNOyUlcONJ",
        "outputId": "896cba6e-162c-49aa-8909-47f846edbbfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: pinecone-client==3.0.0 in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (2025.6.15)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (4.14.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import logging\n",
        "try:\n",
        "    from pinecone import Pinecone, ServerlessSpec\n",
        "    PINECONE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    try:\n",
        "        import pinecone\n",
        "        PINECONE_AVAILABLE = True\n",
        "        print(\"Using legacy Pinecone import\")\n",
        "    except ImportError:\n",
        "        PINECONE_AVAILABLE = False\n",
        "        print(\"Pinecone not available - using FAISS as fallback\")\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "import faiss\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1cPr_zv6WmuO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class RAGConfig:\n",
        "    \"\"\"Configuration class for RAG system\"\"\"\n",
        "    openai_api_key: str = \"\"\n",
        "    pinecone_api_key: str = \"\"\n",
        "    pinecone_environment: str = \"us-east-1\"\n",
        "    index_name: str = \"qa-bot-index\"\n",
        "    embedding_model: str = \"text-embedding-3-large\"\n",
        "    chat_model: str = \"gpt-4-turbo-preview\"\n",
        "    chunk_size: int = 1000\n",
        "    chunk_overlap: int = 200\n",
        "    max_tokens: int = 150000\n",
        "    temperature: float = 0.1\n",
        "    top_k: int = 5\n",
        "    similarity_threshold: float = 0.7\n",
        "\n",
        "#initialize configuration\n",
        "config = RAGConfig()\n",
        "def setup_api_keys():\n",
        "    \"\"\"Setup API keys from environment or user input\"\"\"\n",
        "    try:\n",
        "        #trying to get from environment first\n",
        "        config.openai_api_key = os.getenv('OPENAI_API_KEY', '')\n",
        "        config.pinecone_api_key = os.getenv('PINECONE_API_KEY', '')\n",
        "\n",
        "        #instructions will be provided if not found. i have not included my personal api and pinecone keys for security reasons\n",
        "        if not config.openai_api_key:\n",
        "            print(\"openAI API key not found!\")\n",
        "            print(\"To use OpenAI embeddings and chat completion:\")\n",
        "            print(\"1. Get your API key from https://platform.openai.com/api-keys\")\n",
        "            print(\"2. Set it as: config.openai_api_key = 'your-api-key-here'\")\n",
        "            print(\"3. Or set environment variable: OPENAI_API_KEY\")\n",
        "            print(\"\\nFor demo purposes, we'll use a local embedding model.\")\n",
        "            config.openai_api_key = \"demo-mode\"\n",
        "\n",
        "        if not config.pinecone_api_key:\n",
        "            print(\"pinecone API key not found!\")\n",
        "            print(\"To use Pinecone vector database:\")\n",
        "            print(\"1. Get your API key from https://app.pinecone.io/\")\n",
        "            print(\"2. Set it as: config.pinecone_api_key = 'your-api-key-here'\")\n",
        "            print(\"3. Or set environment variable: PINECONE_API_KEY\")\n",
        "            print(\"\\nFor demo purposes, we'll use FAISS as local vector database.\")\n",
        "            config.pinecone_api_key = \"demo-mode\"\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error setting up API keys: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "xZe9iy8fbYlz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from datetime import datetime\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Advanced document processing with multiple strategies\"\"\"\n",
        "\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "        self.encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "    def process_documents(self, documents: List[str]) -> List[Document]:\n",
        "        \"\"\"Process and chunk documents with metadata\"\"\"\n",
        "        processed_docs = []\n",
        "\n",
        "        for i, doc_content in enumerate(documents):\n",
        "            #here i am creating the document chunks\n",
        "            chunks = self.text_splitter.split_text(doc_content)\n",
        "\n",
        "            for j, chunk in enumerate(chunks):\n",
        "                #adding the metadata here\n",
        "                metadata = {\n",
        "                    \"doc_id\": i,\n",
        "                    \"chunk_id\": j,\n",
        "                    \"chunk_size\": len(chunk),\n",
        "                    \"token_count\": len(self.encoding.encode(chunk)),\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "\n",
        "                processed_docs.append(Document(\n",
        "                    page_content=chunk,\n",
        "                    metadata=metadata\n",
        "                ))\n",
        "\n",
        "        return processed_docs\n",
        "\n",
        "    def extract_keywords(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract keywords from text for enhanced retrieval\"\"\"\n",
        "        #simple keyword extraction (can be enhanced with NLP libraries)\n",
        "        words = text.lower().split()\n",
        "        #filter out common stop words\n",
        "        stop_words = {'the', 'is', 'at', 'which', 'on', 'and', 'a', 'to', 'are', 'as', 'was', 'for', 'with', 'by'}\n",
        "        keywords = [word for word in words if word not in stop_words and len(word) > 3]\n",
        "        return list(set(keywords))"
      ],
      "metadata": {
        "id": "Oob4Ovzod1Rt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingGenerator:\n",
        "    \"\"\"Generate embeddings using OpenAI API with fallback options\"\"\"\n",
        "\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.client = openai.OpenAI(api_key=config.openai_api_key)\n",
        "        #fallback embedding model\n",
        "        self.local_model = None\n",
        "\n",
        "    def initialize_local_model(self):\n",
        "        \"\"\"Initialize local embedding model as fallback\"\"\"\n",
        "        try:\n",
        "            self.local_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            logger.info(\"Local embedding model initialized\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize local model: {e}\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings with retry logic\"\"\"\n",
        "        try:\n",
        "            #primary: OpenAI embeddings\n",
        "            return self._generate_openai_embeddings(texts)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"OpenAI embedding failed: {e}\")\n",
        "            #fallback: Local model\n",
        "            return self._generate_local_embeddings(texts)\n",
        "\n",
        "    def _generate_openai_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings using OpenAI API\"\"\"\n",
        "        if self.config.openai_api_key == \"demo-mode\":\n",
        "            #using local model instead\n",
        "            return self._generate_local_embeddings(texts)\n",
        "\n",
        "        embeddings = []\n",
        "\n",
        "        #processing in batches to handle rate limits\n",
        "        batch_size = 50\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i + batch_size]\n",
        "\n",
        "            response = self.client.embeddings.create(\n",
        "                model=self.config.embedding_model,\n",
        "                input=batch\n",
        "            )\n",
        "\n",
        "            batch_embeddings = [item.embedding for item in response.data]\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _generate_local_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings using local model\"\"\"\n",
        "        if self.local_model is None:\n",
        "            self.initialize_local_model()\n",
        "\n",
        "        embeddings = self.local_model.encode(texts)\n",
        "        #pad to match OpenAI embedding dimension (3072)\n",
        "        padded_embeddings = []\n",
        "        for emb in embeddings:\n",
        "            if len(emb) < 3072:\n",
        "                #pad with zeros\n",
        "                padded = np.pad(emb, (0, 3072 - len(emb)), 'constant')\n",
        "                padded_embeddings.append(padded.tolist())\n",
        "            else:\n",
        "                padded_embeddings.append(emb[:3072].tolist())\n",
        "\n",
        "        return padded_embeddings"
      ],
      "metadata": {
        "id": "x9jhG8eHd7y2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import pinecone\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VectorDatabaseManager:\n",
        "    \"\"\"Unified vector database manager with Pinecone and FAISS support\"\"\"\n",
        "\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.use_pinecone = PINECONE_AVAILABLE and config.pinecone_api_key != \"demo-mode\"\n",
        "        self.index = None\n",
        "        self.faiss_index = None\n",
        "        self.document_store = {}\n",
        "\n",
        "        if self.use_pinecone:\n",
        "            self._initialize_pinecone()\n",
        "        else:\n",
        "            self._initialize_faiss()\n",
        "\n",
        "    def _initialize_pinecone(self):\n",
        "        \"\"\"Initialize Pinecone vector database\"\"\"\n",
        "        try:\n",
        "            #handling different Pinecone versions\n",
        "            if hasattr(pinecone, 'init'):\n",
        "                #this is the legacy version\n",
        "                pinecone.init(\n",
        "                    api_key=self.config.pinecone_api_key,\n",
        "                    environment=self.config.pinecone_environment\n",
        "                )\n",
        "\n",
        "                #checking if index exists\n",
        "                if self.config.index_name not in pinecone.list_indexes():\n",
        "                    pinecone.create_index(\n",
        "                        name=self.config.index_name,\n",
        "                        dimension=3072,\n",
        "                        metric='cosine'\n",
        "                    )\n",
        "                    logger.info(f\"Created Pinecone index: {self.config.index_name}\")\n",
        "\n",
        "                self.index = pinecone.Index(self.config.index_name)\n",
        "\n",
        "            else:\n",
        "                #new version\n",
        "                self.pc = Pinecone(api_key=self.config.pinecone_api_key)\n",
        "\n",
        "                if self.config.index_name not in self.pc.list_indexes().names():\n",
        "                    self.pc.create_index(\n",
        "                        name=self.config.index_name,\n",
        "                        dimension=3072,\n",
        "                        metric='cosine',\n",
        "                        spec=ServerlessSpec(\n",
        "                            cloud='aws',\n",
        "                            region=self.config.pinecone_environment\n",
        "                        )\n",
        "                    )\n",
        "                    logger.info(f\"Created Pinecone index: {self.config.index_name}\")\n",
        "\n",
        "                self.index = self.pc.Index(self.config.index_name)\n",
        "\n",
        "            logger.info(\"Pinecone initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize Pinecone: {e}\")\n",
        "            logger.info(\"Falling back to FAISS\")\n",
        "            self.use_pinecone = False\n",
        "            self._initialize_faiss()\n",
        "\n",
        "    def _initialize_faiss(self):\n",
        "        \"\"\"Initialize FAISS vector database as fallback\"\"\"\n",
        "        try:\n",
        "            #creating FAISS index\n",
        "            self.faiss_index = faiss.IndexFlatIP(3072)\n",
        "            logger.info(\"FAISS index initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize FAISS: {e}\")\n",
        "            raise\n",
        "\n",
        "    def upsert_documents(self, documents: List[Document], embeddings: List[List[float]]):\n",
        "        \"\"\"Upsert documents with embeddings\"\"\"\n",
        "        try:\n",
        "            if self.use_pinecone:\n",
        "                self._upsert_to_pinecone(documents, embeddings)\n",
        "            else:\n",
        "                self._upsert_to_faiss(documents, embeddings)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error upserting documents: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _upsert_to_pinecone(self, documents: List[Document], embeddings: List[List[float]]):\n",
        "        \"\"\"Upsert to Pinecone\"\"\"\n",
        "        vectors = []\n",
        "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
        "            vector = {\n",
        "                \"id\": f\"doc_{i}\",\n",
        "                \"values\": embedding,\n",
        "                \"metadata\": {\n",
        "                    \"text\": doc.page_content,\n",
        "                    **doc.metadata\n",
        "                }\n",
        "            }\n",
        "            vectors.append(vector)\n",
        "\n",
        "        #upsert in batches\n",
        "        batch_size = 100\n",
        "        for i in range(0, len(vectors), batch_size):\n",
        "            batch = vectors[i:i + batch_size]\n",
        "            self.index.upsert(vectors=batch)\n",
        "\n",
        "        logger.info(f\"Upserted {len(vectors)} documents to Pinecone\")\n",
        "\n",
        "    def _upsert_to_faiss(self, documents: List[Document], embeddings: List[List[float]]):\n",
        "        \"\"\"Upsert to FAISS\"\"\"\n",
        "        #converting embeddings to numpy array and normalize for cosine similarity\n",
        "        embeddings_array = np.array(embeddings).astype('float32')\n",
        "        #normalizing vectors for cosine similarity\n",
        "        faiss.normalize_L2(embeddings_array)\n",
        "\n",
        "        #adding to FAISS index\n",
        "        start_id = self.faiss_index.ntotal\n",
        "        self.faiss_index.add(embeddings_array)\n",
        "\n",
        "        #storing metadata\n",
        "        for i, doc in enumerate(documents):\n",
        "            doc_id = start_id + i\n",
        "            self.document_store[doc_id] = {\n",
        "                \"text\": doc.page_content,\n",
        "                **doc.metadata\n",
        "            }\n",
        "\n",
        "        logger.info(f\"Upserted {len(documents)} documents to FAISS\")\n",
        "\n",
        "    def search_similar(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search for similar documents\"\"\"\n",
        "        try:\n",
        "            if self.use_pinecone:\n",
        "                return self._search_pinecone(query_embedding, top_k)\n",
        "            else:\n",
        "                return self._search_faiss(query_embedding, top_k)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error searching similar documents: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _search_pinecone(self, query_embedding: List[float], top_k: int) -> List[Dict]:\n",
        "        \"\"\"Search in Pinecone\"\"\"\n",
        "        results = self.index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                \"id\": match.id,\n",
        "                \"score\": match.score,\n",
        "                \"text\": match.metadata.get(\"text\", \"\"),\n",
        "                \"metadata\": match.metadata\n",
        "            }\n",
        "            for match in results.matches\n",
        "        ]\n",
        "\n",
        "    def _search_faiss(self, query_embedding: List[float], top_k: int) -> List[Dict]:\n",
        "        \"\"\"Search in FAISS\"\"\"\n",
        "        #normalizing query vector\n",
        "        query_vector = np.array([query_embedding]).astype('float32')\n",
        "        faiss.normalize_L2(query_vector)\n",
        "\n",
        "        #performing the search\n",
        "        scores, indices = self.faiss_index.search(query_vector, top_k)\n",
        "\n",
        "        results = []\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            if idx != -1 and idx in self.document_store:  # -1 indicates no result\n",
        "                results.append({\n",
        "                    \"id\": f\"doc_{idx}\",\n",
        "                    \"score\": float(score),\n",
        "                    \"text\": self.document_store[idx][\"text\"],\n",
        "                    \"metadata\": self.document_store[idx]\n",
        "                })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "C1aqDFVCeHK3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "import tiktoken\n",
        "import os\n",
        "import openai\n",
        "import logging\n",
        "\n",
        "class RAGSystem:\n",
        "    \"\"\"Advanced RAG system with multiple retrieval strategies\"\"\"\n",
        "\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.doc_processor = DocumentProcessor(config)\n",
        "        self.embedding_generator = EmbeddingGenerator(config)\n",
        "        self.vector_db = VectorDatabaseManager(config)\n",
        "        self.client = openai.OpenAI(api_key=config.openai_api_key)\n",
        "\n",
        "\n",
        "        self.initialize_system()\n",
        "\n",
        "    def initialize_system(self):\n",
        "        \"\"\"Initialize all system components\"\"\"\n",
        "        try:\n",
        "            logger.info(\"RAG system initialized successfully\")\n",
        "            if self.vector_db.use_pinecone:\n",
        "                logger.info(\"Using Pinecone for vector storage\")\n",
        "            else:\n",
        "                logger.info(\"Using FAISS for vector storage\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing RAG system: {e}\")\n",
        "            raise\n",
        "\n",
        "    def ingest_documents(self, documents: List[str]):\n",
        "        \"\"\"Ingest documents into the system\"\"\"\n",
        "        try:\n",
        "\n",
        "            processed_docs = self.doc_processor.process_documents(documents)\n",
        "\n",
        "            #generating embeddings\n",
        "            texts = [doc.page_content for doc in processed_docs]\n",
        "            embeddings = self.embedding_generator.generate_embeddings(texts)\n",
        "\n",
        "            #storing in vector database\n",
        "            self.vector_db.upsert_documents(processed_docs, embeddings)\n",
        "\n",
        "            logger.info(f\"Successfully ingested {len(processed_docs)} document chunks\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error ingesting documents: {e}\")\n",
        "            raise\n",
        "\n",
        "    def retrieve_relevant_context(self, query: str) -> List[Dict]:\n",
        "        \"\"\"Retrieve relevant context for a query\"\"\"\n",
        "        try:\n",
        "            #gnerating query embedding\n",
        "            query_embedding = self.embedding_generator.generate_embeddings([query])[0]\n",
        "\n",
        "            #searching similar documents\n",
        "            results = self.vector_db.search_similar(\n",
        "                query_embedding,\n",
        "                top_k=self.config.top_k\n",
        "            )\n",
        "\n",
        "            #filtering by similarity threshold\n",
        "            filtered_results = [\n",
        "                result for result in results\n",
        "                if result[\"score\"] >= self.config.similarity_threshold\n",
        "            ]\n",
        "\n",
        "            return filtered_results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving context: {e}\")\n",
        "            return []\n",
        "\n",
        "    def generate_response(self, query: str, context: List[Dict]) -> str:\n",
        "        \"\"\"Generate response using retrieved context\"\"\"\n",
        "        try:\n",
        "            #checking if we have a real OpenAI API key\n",
        "            if self.config.openai_api_key == \"demo-mode\":\n",
        "                #generate a simple rule-based response for demo\n",
        "                return self._generate_demo_response(query, context)\n",
        "\n",
        "            #preparing context text\n",
        "            context_text = \"\\n\\n\".join([\n",
        "                f\"[Document {i+1}] {item['text']}\"\n",
        "                for i, item in enumerate(context)\n",
        "            ])\n",
        "\n",
        "            #preparing prompt\n",
        "            prompt = f\"\"\"\n",
        "            You are an intelligent QA assistant for a business. Use the provided context to answer the question accurately and comprehensively.\n",
        "\n",
        "            Context:\n",
        "            {context_text}\n",
        "\n",
        "            Question: {query}\n",
        "\n",
        "            Instructions:\n",
        "            1. Provide a clear, accurate answer based on the context\n",
        "            2. If the context doesn't contain enough information, say so\n",
        "            3. Be concise but comprehensive\n",
        "            4. Use a professional, helpful tone\n",
        "\n",
        "            Answer:\n",
        "            \"\"\"\n",
        "\n",
        "            #generating response\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.chat_model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful business QA assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=self.config.temperature,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating response: {e}\")\n",
        "            return \"I apologize, but I encountered an error while generating a response.\"\n",
        "\n",
        "    def _generate_demo_response(self, query: str, context: List[Dict]) -> str:\n",
        "        \"\"\"Generate a demo response without OpenAI API\"\"\"\n",
        "        if not context:\n",
        "            return \"I couldn't find relevant information to answer your question. Please try rephrasing your query.\"\n",
        "\n",
        "        #simple keyword matching for demo\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        #extracting relevant text from context\n",
        "        relevant_text = \"\"\n",
        "        for item in context[:2]:  # Use top 2 results\n",
        "            relevant_text += item['text'] + \" \"\n",
        "\n",
        "        #generating simple response based on keywords\n",
        "        if any(word in query_lower for word in ['hours', 'working', 'time']):\n",
        "            return f\"Based on the available information: {relevant_text[:300]}...\"\n",
        "        elif any(word in query_lower for word in ['cost', 'price', 'pricing']):\n",
        "            return f\"Regarding pricing: {relevant_text[:300]}...\"\n",
        "        elif any(word in query_lower for word in ['support', 'help', 'contact']):\n",
        "            return f\"For support information: {relevant_text[:300]}...\"\n",
        "        else:\n",
        "            return f\"Based on the context: {relevant_text[:300]}...\"\n",
        "\n",
        "    def ask_question(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete QA pipeline\"\"\"\n",
        "        try:\n",
        "            #retrieve relevant context\n",
        "            context = self.retrieve_relevant_context(query)\n",
        "\n",
        "            #generate response\n",
        "            answer = self.generate_response(query, context)\n",
        "\n",
        "            #prepare response\n",
        "            response = {\n",
        "                \"question\": query,\n",
        "                \"answer\": answer,\n",
        "                \"context_sources\": len(context),\n",
        "                \"retrieved_context\": context,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in QA pipeline: {e}\")\n",
        "            return {\n",
        "                \"question\": query,\n",
        "                \"answer\": \"I apologize, but I encountered an error while processing your question.\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }"
      ],
      "metadata": {
        "id": "P_E4wxoNeckn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_business_data():\n",
        "    \"\"\"Create sample business documents for demo\"\"\"\n",
        "\n",
        "    sample_docs = [\n",
        "        \"\"\"\n",
        "        Company Overview:\n",
        "        TechCorp Solutions is a leading technology company specializing in cloud computing,\n",
        "        artificial intelligence, and digital transformation services. Founded in 2020, we have\n",
        "        grown to serve over 500 enterprise clients across various industries including healthcare,\n",
        "        finance, and retail. Our mission is to empower businesses through innovative technology\n",
        "        solutions that drive growth and efficiency.\n",
        "\n",
        "        Our core services include:\n",
        "        - Cloud migration and management\n",
        "        - AI/ML consulting and implementation\n",
        "        - Digital transformation strategy\n",
        "        - Cybersecurity solutions\n",
        "        - Data analytics and business intelligence\n",
        "        \"\"\",\n",
        "\n",
        "        \"\"\"\n",
        "        HR Policies and Benefits:\n",
        "\n",
        "        Working Hours: Standard working hours are 9 AM to 6 PM, Monday through Friday.\n",
        "        We offer flexible working arrangements including remote work options and flexible hours.\n",
        "\n",
        "        Leave Policy: Employees are entitled to 25 days of paid vacation annually,\n",
        "        10 sick days, and 3 personal days. We also observe all major holidays.\n",
        "\n",
        "        Benefits Package:\n",
        "        - Comprehensive health insurance (medical, dental, vision)\n",
        "        - 401(k) retirement plan with company matching\n",
        "        - Professional development budget of $2,000 per year\n",
        "        - Wellness programs including gym membership reimbursement\n",
        "        - Life and disability insurance\n",
        "        - Parental leave: 12 weeks paid leave for new parents\n",
        "        \"\"\",\n",
        "\n",
        "        \"\"\"\n",
        "        Product Information:\n",
        "\n",
        "        CloudSync Pro: Our flagship cloud management platform that helps businesses\n",
        "        migrate, monitor, and optimize their cloud infrastructure. Features include\n",
        "        automated scaling, cost optimization, and security monitoring. Pricing starts\n",
        "        at $500/month for small businesses and scales based on usage.\n",
        "\n",
        "        AI Assistant Suite: A comprehensive AI toolkit that includes chatbots,\n",
        "        document processing, and predictive analytics. Perfect for businesses looking\n",
        "        to automate routine tasks and gain insights from their data. Custom pricing\n",
        "        based on requirements.\n",
        "\n",
        "        SecureGuard: Enterprise-grade cybersecurity solution providing real-time\n",
        "        threat detection, vulnerability assessment, and incident response.\n",
        "        Pricing: $50 per user per month.\n",
        "        \"\"\",\n",
        "\n",
        "        \"\"\"\n",
        "        Sales and Support:\n",
        "\n",
        "        Sales Process: Our sales team follows a consultative approach, starting with\n",
        "        a needs assessment, followed by a custom proposal, and implementation planning.\n",
        "        Average sales cycle is 30-45 days for enterprise clients.\n",
        "\n",
        "        Support Channels:\n",
        "        - 24/7 phone support for premium clients\n",
        "        - Email support with 4-hour response time\n",
        "        - Live chat during business hours\n",
        "        - Comprehensive knowledge base and documentation\n",
        "        - Dedicated account managers for enterprise clients\n",
        "\n",
        "        Training and Onboarding: We provide comprehensive training programs for all\n",
        "        our products, including online courses, live workshops, and one-on-one sessions.\n",
        "        \"\"\",\n",
        "\n",
        "        \"\"\"\n",
        "        Financial Information:\n",
        "\n",
        "        Pricing Structure: We offer flexible pricing models including subscription-based,\n",
        "        usage-based, and enterprise licensing. Volume discounts are available for\n",
        "        large organizations.\n",
        "\n",
        "        Payment Terms: Standard payment terms are Net 30 days. We accept various\n",
        "        payment methods including credit cards, wire transfers, and ACH.\n",
        "\n",
        "        Refund Policy: We offer a 30-day money-back guarantee for all new customers.\n",
        "        Refunds are processed within 5-7 business days.\n",
        "\n",
        "        Company Growth: Year-over-year revenue growth of 150% in 2024. We're\n",
        "        expanding into new markets and planning to open offices in Europe and Asia.\n",
        "        \"\"\"\n",
        "    ]\n",
        "\n",
        "    return sample_docs\n",
        "\n",
        "def run_demo():\n",
        "    \"\"\"Run a complete demo of the RAG system\"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"RAG MODEL QA BOT - DEMONSTRATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    #setting up API keys\n",
        "    print(\"\\n1. Setting up API keys...\")\n",
        "    if not setup_api_keys():\n",
        "        print(\"Please configure your API keys to run the demo\")\n",
        "        return\n",
        "\n",
        "    #initializing RAG system\n",
        "    print(\"\\n2. Initializing RAG system...\")\n",
        "    try:\n",
        "        rag_system = RAGSystem(config)\n",
        "        print(\"✓ RAG system initialized successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error initializing RAG system: {e}\")\n",
        "        return\n",
        "\n",
        "    #ingesting sample documents\n",
        "    print(\"\\n3. Ingesting sample business documents...\")\n",
        "    try:\n",
        "        sample_docs = create_sample_business_data()\n",
        "        rag_system.ingest_documents(sample_docs)\n",
        "        print(f\"✓ Successfully ingested {len(sample_docs)} documents\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error ingesting documents: {e}\")\n",
        "        return\n",
        "\n",
        "    #my demo questions\n",
        "    demo_questions = [\n",
        "        \"What are the company's working hours and leave policies?\",\n",
        "        \"How much does CloudSync Pro cost?\",\n",
        "        \"What support channels are available?\",\n",
        "        \"What is the refund policy?\",\n",
        "        \"What are the company's core services?\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n4. Testing QA capabilities...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, question in enumerate(demo_questions, 1):\n",
        "        print(f\"\\nQuestion {i}: {question}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        try:\n",
        "            response = rag_system.ask_question(question)\n",
        "            print(f\"Answer: {response['answer']}\")\n",
        "            print(f\"Sources used: {response['context_sources']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n5. Demo completed successfully!\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "XFtcY1UCe15I"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedRAGFeatures:\n",
        "    \"\"\"Advanced features for production RAG system\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system: RAGSystem):\n",
        "        self.rag_system = rag_system\n",
        "        self.query_history = []\n",
        "        self.performance_metrics = {\n",
        "            \"total_queries\": 0,\n",
        "            \"avg_response_time\": 0,\n",
        "            \"successful_responses\": 0\n",
        "        }\n",
        "\n",
        "    def hybrid_search(self, query: str) -> List[Dict]:\n",
        "        \"\"\"Combine vector and keyword search\"\"\"\n",
        "        #performing vector search\n",
        "        vector_results = self.rag_system.retrieve_relevant_context(query)\n",
        "\n",
        "        #keyyword search\n",
        "        keywords = self.rag_system.doc_processor.extract_keywords(query)\n",
        "\n",
        "        #combining and rank results\n",
        "        combined_results = vector_results\n",
        "\n",
        "        return combined_results\n",
        "\n",
        "    def query_expansion(self, query: str) -> str:\n",
        "        \"\"\"Expand query with related terms\"\"\"\n",
        "        #openAI to generate query variations\n",
        "        try:\n",
        "            response = self.rag_system.client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Generate 3 alternative ways to ask the same question.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Original question: {query}\"}\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                max_tokens=200\n",
        "            )\n",
        "\n",
        "            expanded_query = response.choices[0].message.content\n",
        "            return f\"{query} {expanded_query}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error expanding query: {e}\")\n",
        "            return query\n",
        "\n",
        "    def answer_validation(self, answer: str, context: List[Dict]) -> Dict[str, Any]:\n",
        "        \"\"\"Validate answer quality and relevance\"\"\"\n",
        "        validation_metrics = {\n",
        "            \"confidence_score\": 0.8,\n",
        "            \"context_relevance\": len(context) > 0,\n",
        "            \"answer_completeness\": len(answer) > 50,\n",
        "            \"factual_consistency\": True\n",
        "        }\n",
        "\n",
        "        return validation_metrics\n",
        "\n",
        "    def get_analytics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system analytics and performance metrics\"\"\"\n",
        "        return {\n",
        "            \"performance_metrics\": self.performance_metrics,\n",
        "            \"query_history_count\": len(self.query_history),\n",
        "            \"system_health\": \"operational\"\n",
        "        }"
      ],
      "metadata": {
        "id": "zDXlOvaMe6dL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_demo():\n",
        "    \"\"\"Interactive demo interface\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INTERACTIVE RAG QA BOT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Type 'exit' to quit, 'help' for commands\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        rag_system = RAGSystem(config)\n",
        "        sample_docs = create_sample_business_data()\n",
        "        rag_system.ingest_documents(sample_docs)\n",
        "        advanced_features = AdvancedRAGFeatures(rag_system)\n",
        "\n",
        "        print(\"✓ System ready!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error initializing system: {e}\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nYour question: \").strip()\n",
        "\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            elif user_input.lower() == 'help':\n",
        "                print(\"\\nAvailable commands:\")\n",
        "                print(\"- Ask any question about the business\")\n",
        "                print(\"- 'analytics' - View system analytics\")\n",
        "                print(\"- 'exit' - Quit the demo\")\n",
        "                continue\n",
        "            elif user_input.lower() == 'analytics':\n",
        "                analytics = advanced_features.get_analytics()\n",
        "                print(f\"\\nSystem Analytics: {json.dumps(analytics, indent=2)}\")\n",
        "                continue\n",
        "            elif not user_input:\n",
        "                continue\n",
        "\n",
        "\n",
        "            start_time = datetime.now()\n",
        "            response = rag_system.ask_question(user_input)\n",
        "            end_time = datetime.now()\n",
        "\n",
        "            response_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "\n",
        "            print(f\"\\nAnswer: {response['answer']}\")\n",
        "            print(f\"Response time: {response_time:.2f} seconds\")\n",
        "            print(f\"Sources: {response['context_sources']}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "0fCGdECje-mC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"RAG MODEL QA BOT - BUSINESS IMPLEMENTATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    #configuration check\n",
        "    print(\"Configuration:\")\n",
        "    print(f\"- Embedding Model: {config.embedding_model}\")\n",
        "    print(f\"- Chat Model: {config.chat_model}\")\n",
        "    print(f\"- Chunk Size: {config.chunk_size}\")\n",
        "    print(f\"- Top K Results: {config.top_k}\")\n",
        "\n",
        "    #demo\n",
        "    choice = input(\"\\nChoose demo mode:\\n1. Automated demo\\n2. Interactive demo\\n3. Setup only\\nChoice (1-3): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        run_demo()\n",
        "    elif choice == \"2\":\n",
        "        interactive_demo()\n",
        "    elif choice == \"3\":\n",
        "        print(\"Setup completed. You can now use the RAG system.\")\n",
        "    else:\n",
        "        print(\"Running automated demo...\")\n",
        "        run_demo()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkD9niYRfDVp",
        "outputId": "de9e6c67-5f6e-4094-822c-dcc878caec2a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG MODEL QA BOT - BUSINESS IMPLEMENTATION\n",
            "============================================================\n",
            "Configuration:\n",
            "- Embedding Model: text-embedding-3-large\n",
            "- Chat Model: gpt-4-turbo-preview\n",
            "- Chunk Size: 1000\n",
            "- Top K Results: 5\n",
            "\n",
            "Choose demo mode:\n",
            "1. Automated demo\n",
            "2. Interactive demo\n",
            "3. Setup only\n",
            "Choice (1-3): 3\n",
            "Setup completed. You can now use the RAG system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGEvaluation:\n",
        "    \"\"\"Evaluation framework for RAG system\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system: RAGSystem):\n",
        "        self.rag_system = rag_system\n",
        "\n",
        "    def evaluate_retrieval_quality(self, test_queries: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate retrieval quality metrics\"\"\"\n",
        "        metrics = {\n",
        "            \"avg_retrieval_time\": 0,\n",
        "            \"avg_context_relevance\": 0,\n",
        "            \"retrieval_success_rate\": 0\n",
        "        }\n",
        "\n",
        "        total_time = 0\n",
        "        successful_retrievals = 0\n",
        "\n",
        "        for query in test_queries:\n",
        "            start_time = datetime.now()\n",
        "            context = self.rag_system.retrieve_relevant_context(query)\n",
        "            end_time = datetime.now()\n",
        "\n",
        "            total_time += (end_time - start_time).total_seconds()\n",
        "\n",
        "            if context:\n",
        "                successful_retrievals += 1\n",
        "\n",
        "        metrics[\"avg_retrieval_time\"] = total_time / len(test_queries)\n",
        "        metrics[\"retrieval_success_rate\"] = successful_retrievals / len(test_queries)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_answer_quality(self, test_qa_pairs: List[Dict]) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate answer quality metrics\"\"\"\n",
        "        #this would include metrics like BLEU, ROUGE, etc.\n",
        "        #for demo purposes here i have used simple metrics\n",
        "\n",
        "        metrics = {\n",
        "            \"avg_answer_length\": 0,\n",
        "            \"response_completeness\": 0\n",
        "        }\n",
        "\n",
        "        total_length = 0\n",
        "        complete_responses = 0\n",
        "\n",
        "        for qa_pair in test_qa_pairs:\n",
        "            response = self.rag_system.ask_question(qa_pair[\"question\"])\n",
        "            answer = response[\"answer\"]\n",
        "\n",
        "            total_length += len(answer)\n",
        "\n",
        "            if len(answer) > 50 and \"sorry\" not in answer.lower():\n",
        "                complete_responses += 1\n",
        "\n",
        "        metrics[\"avg_answer_length\"] = total_length / len(test_qa_pairs)\n",
        "        metrics[\"response_completeness\"] = complete_responses / len(test_qa_pairs)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "#testing the evaluation\n",
        "def run_evaluation():\n",
        "    \"\"\"Run evaluation tests\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RAG SYSTEM EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    #initialize system\n",
        "    rag_system = RAGSystem(config)\n",
        "    sample_docs = create_sample_business_data()\n",
        "    rag_system.ingest_documents(sample_docs)\n",
        "\n",
        "    evaluator = RAGEvaluation(rag_system)\n",
        "\n",
        "    #tesst queries\n",
        "    test_queries = [\n",
        "        \"What are the working hours?\",\n",
        "        \"How much does CloudSync Pro cost?\",\n",
        "        \"What is the refund policy?\",\n",
        "        \"What support options are available?\",\n",
        "        \"What are the core services?\"\n",
        "    ]\n",
        "\n",
        "    test_qa_pairs = [\n",
        "        {\"question\": \"What are the working hours?\", \"expected\": \"9 AM to 6 PM\"},\n",
        "        {\"question\": \"What is the refund policy?\", \"expected\": \"30-day money-back guarantee\"}\n",
        "    ]\n",
        "\n",
        "    # evaluations\n",
        "    retrieval_metrics = evaluator.evaluate_retrieval_quality(test_queries)\n",
        "    answer_metrics = evaluator.evaluate_answer_quality(test_qa_pairs)\n",
        "\n",
        "    print(\"\\nRetrieval Metrics:\")\n",
        "    for metric, value in retrieval_metrics.items():\n",
        "        print(f\"- {metric}: {value:.3f}\")\n",
        "\n",
        "    print(\"\\nAnswer Quality Metrics:\")\n",
        "    for metric, value in answer_metrics.items():\n",
        "        print(f\"- {metric}: {value:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "csI6TH-3fH0H"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_system_config():\n",
        "    \"\"\"Export system configuration for deployment\"\"\"\n",
        "    deployment_config = {\n",
        "        \"model_settings\": {\n",
        "            \"embedding_model\": config.embedding_model,\n",
        "            \"chat_model\": config.chat_model,\n",
        "            \"temperature\": config.temperature,\n",
        "            \"chunk_size\": config.chunk_size\n",
        "        },\n",
        "        \"infrastructure\": {\n",
        "            \"pinecone_index\": config.index_name,\n",
        "            \"vector_dimension\": 3072,\n",
        "            \"similarity_threshold\": config.similarity_threshold\n",
        "        },\n",
        "        \"performance\": {\n",
        "            \"max_tokens\": config.max_tokens,\n",
        "            \"top_k\": config.top_k\n",
        "        }\n",
        "    }\n",
        ""
      ],
      "metadata": {
        "id": "w9TTKJqtfQIC"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}